{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17f45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import numpy and pandas libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f01cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import necessary libraries in Pytorch and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b58eca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#we import MinMaxScaler function from sklearn to scale our dataset and also make a instance of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4cd638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_test (batch_size,data,percent,num_predicts):\n",
    "    length_train = int(data.shape[0]*(1-percent))\n",
    "    a1 = length_train//batch_size\n",
    "    length_train = batch_size*a1\n",
    "    train_data = data.iloc[:,1:2].values\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    train_set = []\n",
    "    train_label = []\n",
    "    for i in range(length_train):\n",
    "        train_set.append(train_data[i:i + num_predicts])\n",
    "        train_label.append(train_data[i + num_predicts:i + 2*num_predicts])\n",
    "    \n",
    "    length_test = int(data.shape[0] - length_train - 4*num_predicts)\n",
    "    a2 = length_test//batch_size\n",
    "    length_test = int(batch_size*a2) \n",
    "    test = data.iloc[:,1:2].values\n",
    "    test = scaler.fit_transform(test)\n",
    "    test_data = test[length_train + 2*num_predicts:length_train + length_test + 4*num_predicts]\n",
    "    test_set = []\n",
    "    test_label = []\n",
    "    for j in range(length_test):\n",
    "        test_set.append(test_data[j:j + num_predicts])\n",
    "        test_label.append(test_data[j + num_predicts:j + 2*num_predicts])\n",
    "        \n",
    "    \n",
    "    train_set = np.reshape(np.array(train_set),(np.array(train_set).shape[0], np.array(train_set).shape[1], 1))\n",
    "    train_label = np.reshape(np.array(train_label),(np.array(train_label).shape[0], np.array(train_label).shape[1]))\n",
    "    test_set = np.reshape(np.array(test_set),(np.array(test_set).shape[0], np.array(test_set).shape[1], 1))\n",
    "    test_label = np.reshape(np.array(test_label),(np.array(test_label).shape[0], np.array(test_label).shape[1]))\n",
    "    \n",
    "    return train_set, train_label, test_set, test_label\n",
    "#Here is the function to make train set, test set, and label for each of them from the dataframe. We first want to see how many\n",
    "#We want to find the percent of data used for train set. Then, we find length of train data so that it could divisible by batch \n",
    "#size. Then we scale the dataset and make train set and label for each by iterating through the dataset until and make them lag\n",
    "#by number of predict steps we want until we have desired amount of data for train process. Then, we find the length for test\n",
    "#set and we repeat the same process as forming train set and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "234bacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_label, test_set, test_label = format_train_test(64,new_coin_df,0.2,5)\n",
    "\n",
    "train_set, train_label, test_set, test_label = map(torch.tensor,(train_set, train_label, test_set, test_label))\n",
    "\n",
    "train = TensorDataset(train_set, train_label)\n",
    "train_tensor = DataLoader(train, batch_size = 64, shuffle=False)\n",
    "\n",
    "test = TensorDataset(test_set, test_label)\n",
    "test_tensor = DataLoader(test, batch_size= 64, shuffle=False)\n",
    "\n",
    "#we then make train, test sets, and their labels. Then we convert them to torch tensors and make torch dataset and use DataLoader to \n",
    "#to make data ready for training and testing. Here batch size is 64, percent of test set is 0.2 and 5 predictions steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1577e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, out_size, in_size, hidden_size1, hidden_size2,batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.lstm1 = nn.LSTM(input_size=self.in_size,hidden_size=self.hidden_size1,num_layers=2,batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_size1,hidden_size=self.hidden_size2,num_layers=2,batch_first=True)\n",
    "        self.last = nn.Linear(self.hidden_size2, self.out_size)\n",
    "    def forward(self,x,old_state):\n",
    "        s1,state1 = self.lstm1(x,old_state)\n",
    "        s2,state2 = self.lstm2(s1,state1)\n",
    "        return self.last(s2), state2\n",
    "    def initial_s(self, num_predicts):\n",
    "        return (torch.zeros(2,self.batch_size, self.hidden_size1),\n",
    "                torch.zeros(2,self.batch_size, self.hidden_size1))\n",
    "#Here we make a class of LSTM models. We use two lstm steps with 2 layers each and then the dense layer at the end to output prediction\n",
    "#Here we also need to keep track of the state for LSTM layers. We also make initial states with 2 zeros tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86b6f52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device\n",
    "# we check for the availablibility of GPU to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb85dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_model(5,1,5,5,64)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#we make model, upload it to GPU, make CrossEntropy loss instance as long as the Adam optimizer with learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0592041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_model(\n",
      "  (lstm1): LSTM(1, 5, num_layers=2, batch_first=True)\n",
      "  (lstm2): LSTM(5, 5, num_layers=2, batch_first=True)\n",
      "  (last): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model) #print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1450495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model,train_tensor,test_tensor,criterion,optimizer,device,num_predicts,iterations):\n",
    "    for i in range(iterations):\n",
    "        h0, c0 = model.initial_s(num_predicts)\n",
    "        h, c = h0.to(device), c0.to(device)\n",
    "        for (x,y) in train_tensor:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat, (h, c) = model(x.float(), (h, c))\n",
    "            loss = criterion(y_hat, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            h = h.detach()\n",
    "            c = c.detach()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            h01, c01 = model.initial_s(num_predicts)\n",
    "            h1, c1 = h01.to(device), c01.to(device)\n",
    "            lost_val = 0\n",
    "            for (x1, y1) in test_tensor:\n",
    "                x1, y1 = x1.to(device), y1.to(device)\n",
    "                y_hat1, (h1, c1) = model(x1.float(),(h1,c1))\n",
    "                loss1 = criterion(y_hat1, y1.long())\n",
    "                lost_val += loss1.item()\n",
    "                h1 = h1.detach()\n",
    "                c1 = c1.detach()\n",
    "            lost_val /= len(x1)\n",
    "            print(f\"Iteration {i+1}\\n********************\")\n",
    "            print('The loss for train is: ',loss.item())\n",
    "            print('The loss for test is: ',lost_val)\n",
    "#Here we create function to do the training and validation process. We iterate through number of epochs, make initial state for model\n",
    "#and iterate through each data batch to do prediction, compute the loss and do backward propagation to improve the parameters. In\n",
    "#the same epoch, I also apply the model to make prediction on test dataset to keep track of model performace more closely. I also\n",
    "#print iteration round, train loss, and test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d72567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "********************\n",
      "The loss for train is:  1.580761194229126\n",
      "The loss for test is:  0.2221520785242319\n",
      "Iteration 2\n",
      "********************\n",
      "The loss for train is:  1.4932750463485718\n",
      "The loss for test is:  0.20948922634124756\n",
      "Iteration 3\n",
      "********************\n",
      "The loss for train is:  1.2867605686187744\n",
      "The loss for test is:  0.17983133345842361\n",
      "Iteration 4\n",
      "********************\n",
      "The loss for train is:  0.9603341221809387\n",
      "The loss for test is:  0.1338758636265993\n",
      "Iteration 5\n",
      "********************\n",
      "The loss for train is:  0.6447484493255615\n",
      "The loss for test is:  0.09011944849044085\n",
      "Iteration 6\n",
      "********************\n",
      "The loss for train is:  0.42319363355636597\n",
      "The loss for test is:  0.05946787493303418\n",
      "Iteration 7\n",
      "********************\n",
      "The loss for train is:  0.2900962829589844\n",
      "The loss for test is:  0.04107495490461588\n",
      "Iteration 8\n",
      "********************\n",
      "The loss for train is:  0.2100238800048828\n",
      "The loss for test is:  0.03001663344912231\n",
      "Iteration 9\n",
      "********************\n",
      "The loss for train is:  0.15913185477256775\n",
      "The loss for test is:  0.022973094368353486\n",
      "Iteration 10\n",
      "********************\n",
      "The loss for train is:  0.12470443546772003\n",
      "The loss for test is:  0.018224188592284918\n",
      "Iteration 11\n",
      "********************\n",
      "The loss for train is:  0.1003914326429367\n",
      "The loss for test is:  0.014862543670460582\n",
      "Iteration 12\n",
      "********************\n",
      "The loss for train is:  0.08250055462121964\n",
      "The loss for test is:  0.012393398792482913\n",
      "Iteration 13\n",
      "********************\n",
      "The loss for train is:  0.06892162561416626\n",
      "The loss for test is:  0.010519598610699177\n",
      "Iteration 14\n",
      "********************\n",
      "The loss for train is:  0.05833929777145386\n",
      "The loss for test is:  0.009069892461411655\n",
      "Iteration 15\n",
      "********************\n",
      "The loss for train is:  0.04999297112226486\n",
      "The loss for test is:  0.007929713348858058\n",
      "Iteration 16\n",
      "********************\n",
      "The loss for train is:  0.043275509029626846\n",
      "The loss for test is:  0.007020935590844601\n",
      "Iteration 17\n",
      "********************\n",
      "The loss for train is:  0.0378446951508522\n",
      "The loss for test is:  0.006289021926932037\n",
      "Iteration 18\n",
      "********************\n",
      "The loss for train is:  0.03338613733649254\n",
      "The loss for test is:  0.005694922409020364\n",
      "Iteration 19\n",
      "********************\n",
      "The loss for train is:  0.029710527509450912\n",
      "The loss for test is:  0.005204465560382232\n",
      "Iteration 20\n",
      "********************\n",
      "The loss for train is:  0.026628974825143814\n",
      "The loss for test is:  0.004799044312676415\n",
      "Iteration 21\n",
      "********************\n",
      "The loss for train is:  0.024007119238376617\n",
      "The loss for test is:  0.004459626943571493\n",
      "Iteration 22\n",
      "********************\n",
      "The loss for train is:  0.021793948486447334\n",
      "The loss for test is:  0.004173391906078905\n",
      "Iteration 23\n",
      "********************\n",
      "The loss for train is:  0.019876960664987564\n",
      "The loss for test is:  0.003928293532226235\n",
      "Iteration 24\n",
      "********************\n",
      "The loss for train is:  0.018228158354759216\n",
      "The loss for test is:  0.0037190945586189628\n",
      "Iteration 25\n",
      "********************\n",
      "The loss for train is:  0.016788603737950325\n",
      "The loss for test is:  0.00353782219463028\n",
      "Iteration 26\n",
      "********************\n",
      "The loss for train is:  0.015521044842898846\n",
      "The loss for test is:  0.0033789857116062194\n",
      "Iteration 27\n",
      "********************\n",
      "The loss for train is:  0.014389976859092712\n",
      "The loss for test is:  0.003240113423089497\n",
      "Iteration 28\n",
      "********************\n",
      "The loss for train is:  0.013393355533480644\n",
      "The loss for test is:  0.003118456690572202\n",
      "Iteration 29\n",
      "********************\n",
      "The loss for train is:  0.01250720489770174\n",
      "The loss for test is:  0.0030104548059171066\n",
      "Iteration 30\n",
      "********************\n",
      "The loss for train is:  0.011706091463565826\n",
      "The loss for test is:  0.002915186414611526\n",
      "Iteration 31\n",
      "********************\n",
      "The loss for train is:  0.01097951177507639\n",
      "The loss for test is:  0.002828891185345128\n",
      "Iteration 32\n",
      "********************\n",
      "The loss for train is:  0.010323653928935528\n",
      "The loss for test is:  0.0027519798459252343\n",
      "Iteration 33\n",
      "********************\n",
      "The loss for train is:  0.009727117605507374\n",
      "The loss for test is:  0.002683215032448061\n",
      "Iteration 34\n",
      "********************\n",
      "The loss for train is:  0.009185628965497017\n",
      "The loss for test is:  0.0026211017975583673\n",
      "Iteration 35\n",
      "********************\n",
      "The loss for train is:  0.008689537644386292\n",
      "The loss for test is:  0.0025647445436334237\n",
      "Iteration 36\n",
      "********************\n",
      "The loss for train is:  0.008234431967139244\n",
      "The loss for test is:  0.0025148260610876605\n",
      "Iteration 37\n",
      "********************\n",
      "The loss for train is:  0.0078047094866633415\n",
      "The loss for test is:  0.002468050668539945\n",
      "Iteration 38\n",
      "********************\n",
      "The loss for train is:  0.007422448601573706\n",
      "The loss for test is:  0.0024262496081064455\n",
      "Iteration 39\n",
      "********************\n",
      "The loss for train is:  0.007063031196594238\n",
      "The loss for test is:  0.0023872482270235196\n",
      "Iteration 40\n",
      "********************\n",
      "The loss for train is:  0.006731911096721888\n",
      "The loss for test is:  0.002352181480091531\n",
      "Iteration 41\n",
      "********************\n",
      "The loss for train is:  0.006420210003852844\n",
      "The loss for test is:  0.002320480823982507\n",
      "Iteration 42\n",
      "********************\n",
      "The loss for train is:  0.006129265297204256\n",
      "The loss for test is:  0.0022909128965693526\n",
      "Iteration 43\n",
      "********************\n",
      "The loss for train is:  0.005863002967089415\n",
      "The loss for test is:  0.0022641474206466228\n",
      "Iteration 44\n",
      "********************\n",
      "The loss for train is:  0.005609758198261261\n",
      "The loss for test is:  0.0022395145060727373\n",
      "Iteration 45\n",
      "********************\n",
      "The loss for train is:  0.005376017186790705\n",
      "The loss for test is:  0.002216581633547321\n",
      "Iteration 46\n",
      "********************\n",
      "The loss for train is:  0.005156008061021566\n",
      "The loss for test is:  0.0021953165414743125\n",
      "Iteration 47\n",
      "********************\n",
      "The loss for train is:  0.004951958078891039\n",
      "The loss for test is:  0.002176486043026671\n",
      "Iteration 48\n",
      "********************\n",
      "The loss for train is:  0.004751029424369335\n",
      "The loss for test is:  0.002158526171115227\n",
      "Iteration 49\n",
      "********************\n",
      "The loss for train is:  0.00456933444365859\n",
      "The loss for test is:  0.002142503311915789\n",
      "Iteration 50\n",
      "********************\n",
      "The loss for train is:  0.004389014560729265\n",
      "The loss for test is:  0.0021269322678563185\n",
      "Iteration 51\n",
      "********************\n",
      "The loss for train is:  0.004232350736856461\n",
      "The loss for test is:  0.002113178270519711\n",
      "Iteration 52\n",
      "********************\n",
      "The loss for train is:  0.0040725586004555225\n",
      "The loss for test is:  0.0021000559790991247\n",
      "Iteration 53\n",
      "********************\n",
      "The loss for train is:  0.003926803357899189\n",
      "The loss for test is:  0.0020887112041236833\n",
      "Iteration 54\n",
      "********************\n",
      "The loss for train is:  0.0037876316346228123\n",
      "The loss for test is:  0.0020771616109414026\n",
      "Iteration 55\n",
      "********************\n",
      "The loss for train is:  0.003655108157545328\n",
      "The loss for test is:  0.002067314457235625\n",
      "Iteration 56\n",
      "********************\n",
      "The loss for train is:  0.0035305351484566927\n",
      "The loss for test is:  0.002058082878647838\n",
      "Iteration 57\n",
      "********************\n",
      "The loss for train is:  0.003411443205550313\n",
      "The loss for test is:  0.002049568687652936\n",
      "Iteration 58\n",
      "********************\n",
      "The loss for train is:  0.00329807517118752\n",
      "The loss for test is:  0.002041747109615244\n",
      "Iteration 59\n",
      "********************\n",
      "The loss for train is:  0.0031877816654741764\n",
      "The loss for test is:  0.0020344807671790477\n",
      "Iteration 60\n",
      "********************\n",
      "The loss for train is:  0.003084595315158367\n",
      "The loss for test is:  0.002027441376412753\n",
      "Iteration 61\n",
      "********************\n",
      "The loss for train is:  0.0029857070185244083\n",
      "The loss for test is:  0.0020215327858750243\n",
      "Iteration 62\n",
      "********************\n",
      "The loss for train is:  0.002889314666390419\n",
      "The loss for test is:  0.0020158092411293183\n",
      "Iteration 63\n",
      "********************\n",
      "The loss for train is:  0.002802362432703376\n",
      "The loss for test is:  0.0020108515200263355\n",
      "Iteration 64\n",
      "********************\n",
      "The loss for train is:  0.0027190663386136293\n",
      "The loss for test is:  0.0020060727765667252\n",
      "Iteration 65\n",
      "********************\n",
      "The loss for train is:  0.002633498515933752\n",
      "The loss for test is:  0.0020018143331981264\n",
      "Iteration 66\n",
      "********************\n",
      "The loss for train is:  0.00255408463999629\n",
      "The loss for test is:  0.0019977210940851364\n",
      "Iteration 67\n",
      "********************\n",
      "The loss for train is:  0.002477408852428198\n",
      "The loss for test is:  0.0019944413543271367\n",
      "Iteration 68\n",
      "********************\n",
      "The loss for train is:  0.0024047880433499813\n",
      "The loss for test is:  0.0019911754861823283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69\n",
      "********************\n",
      "The loss for train is:  0.0023341327905654907\n",
      "The loss for test is:  0.0019883093518728856\n",
      "Iteration 70\n",
      "********************\n",
      "The loss for train is:  0.0022670684847980738\n",
      "The loss for test is:  0.001985856240935391\n",
      "Iteration 71\n",
      "********************\n",
      "The loss for train is:  0.002202586503699422\n",
      "The loss for test is:  0.0019836069222947117\n",
      "Iteration 72\n",
      "********************\n",
      "The loss for train is:  0.0021408218890428543\n",
      "The loss for test is:  0.0019816879103018437\n",
      "Iteration 73\n",
      "********************\n",
      "The loss for train is:  0.0020813082810491323\n",
      "The loss for test is:  0.0019798761459242087\n",
      "Iteration 74\n",
      "********************\n",
      "The loss for train is:  0.0020231143571436405\n",
      "The loss for test is:  0.0019785747717833146\n",
      "Iteration 75\n",
      "********************\n",
      "The loss for train is:  0.0019691598135977983\n",
      "The loss for test is:  0.001977394291316159\n",
      "Iteration 76\n",
      "********************\n",
      "The loss for train is:  0.0019148109713569283\n",
      "The loss for test is:  0.001976249657673179\n",
      "Iteration 77\n",
      "********************\n",
      "The loss for train is:  0.0018642202485352755\n",
      "The loss for test is:  0.0019754176901187748\n",
      "Iteration 78\n",
      "********************\n",
      "The loss for train is:  0.0018138119485229254\n",
      "The loss for test is:  0.001974949342184118\n",
      "Iteration 79\n",
      "********************\n",
      "The loss for train is:  0.0017658050637692213\n",
      "The loss for test is:  0.0019747236037801486\n",
      "Iteration 80\n",
      "********************\n",
      "The loss for train is:  0.001719157793559134\n",
      "The loss for test is:  0.00197435678455804\n",
      "Iteration 81\n",
      "********************\n",
      "The loss for train is:  0.0016750568756833673\n",
      "The loss for test is:  0.0019743481589102885\n",
      "Iteration 82\n",
      "********************\n",
      "The loss for train is:  0.0016332289669662714\n",
      "The loss for test is:  0.001974478027477744\n",
      "Iteration 83\n",
      "********************\n",
      "The loss for train is:  0.0015891302609816194\n",
      "The loss for test is:  0.001974544598851935\n",
      "Iteration 84\n",
      "********************\n",
      "The loss for train is:  0.0015498690772801638\n",
      "The loss for test is:  0.0019750249848584644\n",
      "Iteration 85\n",
      "********************\n",
      "The loss for train is:  0.0015100184828042984\n",
      "The loss for test is:  0.0019756355959543725\n",
      "Iteration 86\n",
      "********************\n",
      "The loss for train is:  0.0014738013269379735\n",
      "The loss for test is:  0.001976404566448764\n",
      "Iteration 87\n",
      "********************\n",
      "The loss for train is:  0.001435494632460177\n",
      "The loss for test is:  0.001977098057977855\n",
      "Iteration 88\n",
      "********************\n",
      "The loss for train is:  0.0014002446550875902\n",
      "The loss for test is:  0.001977934909518808\n",
      "Iteration 89\n",
      "********************\n",
      "The loss for train is:  0.0013662579003721476\n",
      "The loss for test is:  0.0019791730683209607\n",
      "Iteration 90\n",
      "********************\n",
      "The loss for train is:  0.0013327274937182665\n",
      "The loss for test is:  0.0019801838552666595\n",
      "Iteration 91\n",
      "********************\n",
      "The loss for train is:  0.0013004614738747478\n",
      "The loss for test is:  0.001981392038942431\n",
      "Iteration 92\n",
      "********************\n",
      "The loss for train is:  0.0012699586804956198\n",
      "The loss for test is:  0.001982849837077083\n",
      "Iteration 93\n",
      "********************\n",
      "The loss for train is:  0.0012379251420497894\n",
      "The loss for test is:  0.001984323100259644\n",
      "Iteration 94\n",
      "********************\n",
      "The loss for train is:  0.001209090230986476\n",
      "The loss for test is:  0.0019857223760482157\n",
      "Iteration 95\n",
      "********************\n",
      "The loss for train is:  0.0011810909491032362\n",
      "The loss for test is:  0.0019871734257321805\n",
      "Iteration 96\n",
      "********************\n",
      "The loss for train is:  0.0011530265910550952\n",
      "The loss for test is:  0.0019890776893589646\n",
      "Iteration 97\n",
      "********************\n",
      "The loss for train is:  0.0011261493200436234\n",
      "The loss for test is:  0.001991048422496533\n",
      "Iteration 98\n",
      "********************\n",
      "The loss for train is:  0.0010993056930601597\n",
      "The loss for test is:  0.0019928078072553035\n",
      "Iteration 99\n",
      "********************\n",
      "The loss for train is:  0.0010740214493125677\n",
      "The loss for test is:  0.0019945840249420144\n",
      "Iteration 100\n",
      "********************\n",
      "The loss for train is:  0.0010493205627426505\n",
      "The loss for test is:  0.0019968878223153297\n"
     ]
    }
   ],
   "source": [
    "train_test(model,train_tensor,test_tensor,criterion,optimizer,device,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229bb97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
