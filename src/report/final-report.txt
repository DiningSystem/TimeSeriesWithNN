0. Abstract
    For the final project of CSC240, we look to apply deep learning techniques in the subject of time series analysis and analyze its performance in this type of task, and possibly compare their accuracy against autoregressive methods, which are algorithms specially developed for time series data. Specifically, we look at whether neural networks can be applied in the financial sectors by using a variety of networks such as bidirectional RNN, GRU, and LSTM to predict future prices of Microsoft stocks, Bitcoin, and similar datasets with varying level of features. We have also prepared and generated artificial time series datasets that simulates reality to see how deep learning can be applied in this kind of task. We find that the all four of the networks we experimented with perform relatively well on our datasets, and that there are some networks that perform better at one dataset than the other. 


1. Introduction
    a. Briefly about time series
        Oxford Languages defines "time series" to be "a series of values of a quantity obtained at successive times, often with equal intervals between them"[1], and in mathematics, a time series is "a series of data points indexed (or listed or graphed) in time order"[2]. Definition asides, time series data appears in every part of our life. From every day's temperature and rain level in the East Coast of the United States, to the stock price of AAPL or Ethereum fluctuating every time the market opens, each and everyone of us have seen and directly experienced this type of data. Interested in this simple-sounding yet difficult to understand, for our final project we decide to analyze time series, and specifically, time series prediction.
        
    b. Time series prediction and analysis
        People have always wanted to see and predict the future. Looking at a chart of time series data, it is inevitable that each one of us has wondered at least once about what tomorrow will be like. How will the data behave? Will it go up, or will it go down? Having the ability to predict future would be great! We would be able to know about what tomorrow's temperature will be during our picnic trip, or whether this cryptocurrency that we have invested our entire life savings in will make us appear in the next Forbes 30 under 30, or will it just pull us deeper in the red. However, accurately predicting the future is an impossible thing to do, but with the power of mathematics, we can at least make an educated guess.
        
        Predicting the future is not just for getting rich quick in the stock market; but rather, time series prediction can help researcher in the medical field predict the number of traumas of each person[3], or hospitals in Southern Taiwan to forecast emergency visits[4], or farmers in Iddukki district, Kerala to predict rainfalls [5]. Due to it varying use in all fields, the subject of time series forecasting is widely researched, all the way back in the later half of 20th century, with many algorithms having been developed, and widely used, for this kind of task, most notably autoregressive methods.

    c. Deep learning in time series forecasting
        With autoregerssive methods seem to be the go-to algorithms for this task, our team decides to take a step back and approach this problem in a different way: using deep learning. Consider that the task of time series prediction is taking an input of time series data and output future price, this is the perfect task for a neural network to train on and perform predictions based on the trend learned from the training process. We noticed a specific type of neural network with structures very suitable for the tasks of learning from long series of data: recurrent neural network (RNN). With context of one state being included in the next state of the training process, we think that this would allow the RNN to pick up on historical trends, and from that give a good educated guess.

        With that said, we are interested in seeing the performance of RNN and its variants in the price prediction task in financial datasets, the extent of which some techniques can be used and their effectiveness, and how well they work in both real-life time series dataset and artificial ones. Specifically, we are comparing performance of four different RNNs structure, namely traditional RNN, bidirectional RNN, GRU, and LSTM on 6 datasets, 4 real-world datasets including Microsoft stock price at market close, S&P500 Index, crude oil prices, and Bitcoin price, and 2 artificially generated datasets.

2. Literature Review
    The application of deep learning into the time series prediction is not novel: Brownlee (2018)[6] details on how to construct a deep learning network for this purpose using a wide variety of techniques including Multilayer Perceptrons, Convolutional Neural Networks, and Recurrent Neural Network to predict many different types of datasets, Gamboa (2017) explores similar approaches to time series dataset with deep learning [7], Qiu et al. (2014) proposes a deep learning belief networks for regression and time series forecasting and also experiments with support vector regression [8]. Deep learning in financial datasets has even been explored for over a decade, way back in 2005 by finance researchers in both the academic world and the industry [9], and many other in different fields, such as healthcare (Miotto et al. (2017) [10], Kaushik et al. (2020) [11], Purushotham et al. (2018)), agriculture (Guillén-Navarro et al. (2020) [13], Jin et al. (2020) [14], Atef et al. (2021) [16]), weather (Salman et al. (2015) [16], Hossain et al. (2015) [17])...

    Individually, there has also been many papers individually look at the performance of each of the variant of RNN.
    a) LSTM
        Althelaya et al. (2018) [8] utilizes a bidirectional LSTM structure (a bidirectional LSTM being a two-way LSTM with information from hidden states taken in from both the the previous and the future states) to perform prediction on the S&P 500 Index, and finds that this structure perform well on the applied datasets, with final loss of 0.03247 with MAE (Mean Absolute Error) loss and 0.04211 with RMSE (Root-mean-square Deviation) loss. 

    b) GRU
        In research from Shen et al. (2018), a 3-layer GRU model with input layer of 240 timesteps and 25 hidden neurons has been implemented for multi-class classification. Its performance on the S&P 500 Index achieved an accuracy of 51.4%. The author confirmed that it is effective to use GRU to extract useful information from financial time-series data. Similarly, Dutta et al. implemented a simple GRU with 50 hidden neurons and one dense layer with one hidden neuron on the exact same Bitcoin price dataset. The size of the input layer depending on the lookback ranged from 15 to 60 days. The regression achieved a RMSE of 0.01 on the training set and 0.019 on the testing set. Therefore, these two studies proved that GRU is a valid method for learning and predicting non-linear patterns in the complex financial data. In addition, as Dey et al. (2021) pointed out in their comparison analysis of RNN in stock price prediction, besides outperforming the simple RNN, GRU even produced a better loss than LSTM especially when the lookback period gets shorter.

3. Methodology
    For our comparison, we are using 4 deep learning models:
        1. Traditional RNN
        2. Bidirectional RNN
        3. Gated Recurrent Unit (GRU)
        4. Long-Short Term Memory (LSTM)
    The reasons we chose these specific RNNs and their variants are detailed in the 4 section below, along with the details and the construction of these neural networks. 

    a. Why RNN?
        Recurrent Neural Networks (RNNs) are a class of neural networks that “are naturally suited to processing time-series data and other sequential data” (DiPietro & Hager, 2020). Traditional RNN is the base model of the other RNN variants. Its basic structure is shown in the diagram:
        
        (diagram)
        
        it takes into a sequential input vector, does calculations for the hidden units, and then gives an output vector based on the calculations. The hidden units are the key to enable processing sequential data since the current hidden state is calculated based on the input vector and the previous state. This dependency memorizes the historical data and preserves the order of the sequential data. A sigmoid function is used in hidden states calculations in order to introduce non-linearity to our model as real-life data is usually non-linear. The weight matrix is found by finding gradients during the backpropagation process to minimize the loss function of choice. The same function and set of parameters will be used for calculating all the hidden states as shown in the equation diagram.

    b. Bidirectional RNN
        Bidirectional RNN, as its name suggests, is a recurrent neural network whose information flows both way during the training process. First introduced in 1997 in the paper by Schuster and Paliwal (1997), bidirectional RNN is a direct extension of the original recurrent neural network, with the ability to learn from both past and future contexts due to its two-way nature. As noted by the authors, the structure of the bidirectional RNN contains a two set of state neurons of a traditional RNN that are responsible for either  forward and backward states. Inputs are fed to both context flow, but outputs from forward are not given to inputs of backward states, and vice versa. (see diagram, taken from the paper)
        
        (bidirectional RNN diagram)

        The authors remarked that since the network are considering both past and future information, it allows the objective function to be directly minimized "without the need for delays to include future information", in contrast to the regular undirectional RNN. Experiments with bidirectional RNN confirms its better performance than the regular RNN in some cases (Cao et al. (2018) proposes BRITS (Bidirectional Recurrent Imputation for Time Series), a slight modification of bidirectional RNN, with a 11.56 MAE loss, lower than the 14.24 of RNN), and we speculate that the bidirectional RNN would work better in our time series prediction tasks than the traditional RNN, as well, due to its ability to pick up more context (from both future and past).

    c. Gated Recurrent Unit (GRU)
        GRU is a gating mechanism developed from RNN by Cho et al. in 2014. Every hidden state contains two gates: the forget gate decides how much information in the past hidden state to forget and the update decides how much past information to retain. As shown in the equation, gates are calculated through sigmoid function to have an output ranging from 0 to 1. Thus, we can consider the output from the gate as a percentage of information from the past hidden states. Gating mechanism addresses the problem of vanishing gradients in traditional RNN by storing the information of past state in the two gates, and it also has less computational complexity than LSTM.
            zt = σ(W(z)xt + U(z)ht−1 + b(z))
            rt = σ(W(r)xt + U(r)ht−1 + b(r))
            h ̃t = tanh(Wxt + rt ⊙ Uht−1 + b(h))
            rt = zt ⊙ht−1 +(1−zt)⊙h ̃t)

    d. Long-Short Term Memory (LSTM)
        The idea of LSTM is developed by Sepp et al. in 1997. Here, we refer use the modern paper to explain the mechanism of LSTM. The LSTM has one additional element comparing with the hidden state and it is the cell memory, so it could store both long-term and short-term information. There are three gates in the LSTM cells. They are input, forget, and output gates. The input gate decides how to handle the hidden state from previous cell and the input of the current cell. The output gate decides how to forward the information of the cell to next cell. The forget gate decides which information the cell could ignore and place more emphasis on more important ones. 

4. Network Implementation
    a. Traditional RNN
        We implemented a 1-layer traditional RNN model with 64 hidden neurons on the six datasets by using PyTorch packages. The inputs are grouped in batches to facilitate the backpropagation process. For datasets with large numbers of samples - artificial 1, bitcoin, and artificial 2 datasets - we group them into 64 batches, while for Microsoft stock, crude oil, and S&P 500 stock datasets, we use 32 batches. To improve the robustness of our model and prevent overfitting, we used a dropout rate of 0.2 and the RELU activation function to train our model. Moreover, we did 50 epochs on the S&P 500 stock dataset and 20 epochs on the other datasets since the S&P 500 dataset needed more epochs to converge during backpropagation. 

    b. Bidirectional RNN
        The structure of the bidirectional RNN is very similar to the above traditional RNN, but with the RNN cell replaced with a bidirectional RNN cells. We also apply some changes in the internal linear output layers to take in double the hidden output size, since the output of the bidirectional RNN are doubled in size compared to the RNN. The internal bidirectional RNN cell contains 64 hidden layers. This neural network trains on 50 epoch at a learning rate of 0.001, with an input batch size of 32 or 64 based on sample size, and a lookback of 5 days. 

    c. GRU
        A one-layer GRU was implemented with an input size equal to the lookback of 5 days and 64 hidden neurons in one hidden state. One dense layer of linear function followed it to output the final prediction. Using the TensorDataset and DataLoader method from Pytorch, we transformed the data to tensor and separated them with different batch sizes 32 or 64 based on the sample size. With a learning rate of 0.001, the model was run separately using different loss functions for backpropagation to check the consistency of the prediction results.

    d. LSTM
        We develop LSTM model by Pytorch. We use the first LSTM layer with purpose on mapping input features to the first hidden features. Then we connect that to the second LSTM layer to map to the second hidden features. Then, we connect every feature at the end through a dense layer to output the next number of steps we want. We also initialize hidden states to zero to begin the training then keep propagating the hidden states to later training batches. With a learning rate of 0.001, the model was run separately using different loss functions for backpropagation to check the consistency of the prediction results. We output the loss of train and test set through the number of epochs to see general trend in training. We also plot the predicted values to compare with the test values to visualize that. We apply training, testing, and plotting for every dataset.

    e. Loss Metric
        We used two different loss functions, MSE and L1 (MAE), to do backpropagation and recorded the losses separately to compare the performances. Doing so, we believe, would result in a fairer comparison between the four networks, and it can also give us a more unbiased outlook into the analysis.

5. Data Application
        To test the performance of the four deep learning models we have listed above, we decide to use four financial datasets that come from three different fields, and also two more artificial datasets we generate with certain constraints. We prepare four real-life datasets which come from slightly different financial contexts, whose subfields and metadata we listed in the table below:

        \begin{table}[h!] \centering
            \caption{Datasets }
            \begin{threeparttable}
                    \begin{tabular}{|c||c|c|}
                        \hline
                        Dataset name & Field & Source\\
                        \hline
                        Microsoft Stock Prices & Stock market & Kaggle\tablefootnote{\url{https://www.kaggle.com/vijayvvenkitesh/microsoft-stock-time-series-analysis}}\\
                        S\&P 500 Index & Stock market & DataHub\tablefootnote{\url{https://datahub.io/core/s-and-p-500}}\\
                        Europe Crude Oil Prices & Natural resources & FRED Economic Data\tablefootnote{\url{https://fred.stlouisfed.org/series/DCOILBRENTEU}}\\
                        Bitcoin Prices & Cryptocurrency & Kaggle\tablefootnote{\url{https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory}}\\
                        \hline
                    \end{tabular}
            \end{threeparttable}
        \end{table}

        With the four datasets belonging into slightly different subfields of the financial world, we can reliably compare our prediction accuracy of each models we use and apply our findings to generalize without biases. This following subsections detail some analysis on the above four datasets:

        1. MSFT stock price: For this dataset, we decide to go with the close stock prices of Microsoft (MSFT on NASDAQ) from 2015-04-01 to 2021-03-3. Here we graph the values of Microsoft Stock prices over time in Fig. 1:

        (MSFT Dataset trend)

        As observed in the above trend, we can see that MSFT Stock prices experiences a general increase from 2015, with only small sudden decreases happening in 2018 and 2020. This dataset provides us with a generally increasing time series data, and we expect that our 4 models can easily pick up this forward trend and predict well in during their evaluation. 
 
        2. S&P 500 Index: The Standard \& Poor 500 Index (S\&P 500) is a stock market index tracking the performance of 500 large companies listed on stock exchanges in the United States\footnote{Wikipedia}, and the dataset that we have contains index from 1871-01-01 to 2018-04-01. Here we graph the values of the S\&P 500 Index over time in Fig. 2:

        (S&P500 Index trend)

        The reason we chose this datasets is due to its special feature: it plateaus for quite a while from 1871 to 1987, before sharply increasing and decreasing over the next 40 years, and increasing rapidly when closer to current present. This feature is considerably different from what we observe in the MSFT stock price, where it is a general increase with small fluctuations, giving us another perspective to weigh in in our comparison.

        3. Europe Crude Oil Prices: The dataset contains the crude oil prices (in USD) of Europe from 2015-11-02 to 2021-11-01. Here we graph the values of the Europe crude oil prices over time in Fig. 3:

        (Crude oil prices trend)

        This dataset contains even more different trend than the previous two, with high degree of fluctation with one period of sharp increase and one period of sharp decrease back-to-back, mixed with many fluctuating plateaus also. There are no general upward or downward trend to conclude definitively. With this high entropy, we expect the 4 methods to struggle quite a bit picking up trends in this datasets. 

        4. Bitcoin Prices: The main dataset from Kaggle offer prices of many different cryptocurrency; however, we decided to go with Bitcoin since it seems to be the most popular one currently as of 2021. The dataset contains Bitcoin prices (in USD) at close from 2013-04-29 to 2021-07-06. Here we graph the values of the Bitcoin prices at close over time in Fig. 4:

        (Bitcoin prices trend)

        This dataset has somewhat similar trend to the S&P 500 one; however, after a period of plateau from 2013 to late 2016, it is followed by a small sharp increase, then followed by a small fluctuating plateau, for 2 years, then spiking again before dropping sharply again, while the S&P 500 contains two back-to-back series of spikes and decreases. We also expect the four methods to struggle on confidently picking up the trend as well, consider a quite unorthodox trend growth.

    Besides the real-life datasets listed above, we have also created an algorithm that generated artificial time series dataset, and would apply our neural networks on two which will be created during the course of our final project. Fig 5 and 6 showcase two artificial datasets that were generated by our methods.

    (artificial data, REMEMBER TO USE FROM OUR OWN, NOT FROM THE PROSPECTUS)

    (artificial data, REMEMBER TO USE FROM OUR OWN, NOT FROM THE PROSPECTUS)

    To generate an artificial time-series dataset, we firstly used the Pandas package to create a dataframe ranging from 2000-01-01 to 2021-12-31,  and random numbers generated by Numpy were assigned to the dataframe. Then, the seasonal decomposition method from statsmodels package was applied to the dataframe with a period of 365 days, resulting in trend, seasonality, and residual. The trend was calculated by moving averaging with a window length of 365, so the first half year and the last half year will disappear. We extracted the trend data as our final artificial dataset since it is more similar to the normal financial time-series data. A total of 7307 timesteps was generated.

6. Results
7. Division of Labor

[1] Oxford Languages
[2] https://en.wikipedia.org/wiki/Time_series
[3] https://www.researchgate.net/publication/334960185_Predicting_number_of_traumas_using_the_seasonal_time_series_model
[4] https://pubmed.ncbi.nlm.nih.gov/29196487/
[5] https://www.researchgate.net/publication/328495188_Time-series_Analysis_and_Forecasting_of_Rainfall_at_Idukki_district_Kerala_Machine_Learning_Approach
[6] https://books.google.com/books?hl=en&lr=&id=o5qnDwAAQBAJ&oi=fnd&pg=PP1&dq=deep+learning+in+time+series+forecasting&ots=yH29xMvg2b&sig=wI2SuEDxZZG_VVhWIoTBVwPGMaQ#v=onepage&q=deep%20learning%20in%20time%20series%20forecasting&f=false
[7] https://arxiv.org/abs/1701.01887
[8] https://ieeexplore.ieee.org/abstract/document/7015739
[9] https://www.sciencedirect.com/science/article/pii/S1568494620301216
[10] https://doi.org/10.1093/bib/bbx044
[11] https://doi.org/10.3389/fdata.2020.00004
[12] https://doi.org/10.1016/j.jbi.2018.04.007
[13] https://doi.org/10.3233/AIS-200546
[14] https://doi.org/10.3390/s20051334
[15] https://doi.org/10.1109/MWSCAS47672.2021.9531929
[16] https://doi.org/10.1109/ICACSIS.2015.7415154
[17] https://doi.org/10.1109/IJCNN.2015.7280812
[18] https://ieeexplore.ieee.org/abstract/document/8355458 